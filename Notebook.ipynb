{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OilyGiant Mining Company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In today's dynamic and competitive oil industry, the ability to strategically locate and develop new oil wells is crucial for companies like OilyGiant Mining Company to maximize profitability and ensure sustainable operations. The success of such endeavors heavily relies on the careful analysis of oil well characteristics, including oil quality and reserve volumes, within different geographic regions. By leveraging advanced predictive modeling techniques, such as machine learning, we can accurately estimate reserve volumes for potential new wells and identify promising locations for drilling. This project aims to utilize oil sample data from three distinct regions, coupled with known well parameters, to develop a robust model that not only pinpoints the region with the highest potential profit margin but also assesses associated risks using innovative methodologies like Bootstrapping. Ultimately, this research will provide valuable insights and actionable recommendations to guide decision-making in selecting the optimal site for a new oil well, contributing to the continued success and growth of OilyGiant Mining Company in the oil exploration and production sector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "from scipy import stats as st\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data was sucessfully loaded\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    reg_data_0 = pd.read_csv(\"./data/geo_data_0.csv\")\n",
    "    reg_data_1 = pd.read_csv(\"./data/geo_data_1.csv\")\n",
    "    reg_data_2 = pd.read_csv(\"./data/geo_data_2.csv\")\n",
    "except FileNotFoundError as f_error:\n",
    "    print(f\"The following error: ({f_error}) occured while loading datasets\")\n",
    "else:\n",
    "    print(\"The data was sucessfully loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>txEyH</td>\n",
       "      <td>0.705745</td>\n",
       "      <td>-0.497823</td>\n",
       "      <td>1.221170</td>\n",
       "      <td>105.280062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2acmU</td>\n",
       "      <td>1.334711</td>\n",
       "      <td>-0.340164</td>\n",
       "      <td>4.365080</td>\n",
       "      <td>73.037750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>409Wp</td>\n",
       "      <td>1.022732</td>\n",
       "      <td>0.151990</td>\n",
       "      <td>1.419926</td>\n",
       "      <td>85.265647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>iJLyR</td>\n",
       "      <td>-0.032172</td>\n",
       "      <td>0.139033</td>\n",
       "      <td>2.978566</td>\n",
       "      <td>168.620776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Xdl7t</td>\n",
       "      <td>1.988431</td>\n",
       "      <td>0.155413</td>\n",
       "      <td>4.751769</td>\n",
       "      <td>154.036647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wX4Hy</td>\n",
       "      <td>0.969570</td>\n",
       "      <td>0.489775</td>\n",
       "      <td>-0.735383</td>\n",
       "      <td>64.741541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tL6pL</td>\n",
       "      <td>0.645075</td>\n",
       "      <td>0.530656</td>\n",
       "      <td>1.780266</td>\n",
       "      <td>49.055285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BYPU6</td>\n",
       "      <td>-0.400648</td>\n",
       "      <td>0.808337</td>\n",
       "      <td>-5.624670</td>\n",
       "      <td>72.943292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>j9Oui</td>\n",
       "      <td>0.643105</td>\n",
       "      <td>-0.551583</td>\n",
       "      <td>2.372141</td>\n",
       "      <td>113.356160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OLuZU</td>\n",
       "      <td>2.173381</td>\n",
       "      <td>0.563698</td>\n",
       "      <td>9.441852</td>\n",
       "      <td>127.910945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id        f0        f1        f2     product\n",
       "0  txEyH  0.705745 -0.497823  1.221170  105.280062\n",
       "1  2acmU  1.334711 -0.340164  4.365080   73.037750\n",
       "2  409Wp  1.022732  0.151990  1.419926   85.265647\n",
       "3  iJLyR -0.032172  0.139033  2.978566  168.620776\n",
       "4  Xdl7t  1.988431  0.155413  4.751769  154.036647\n",
       "5  wX4Hy  0.969570  0.489775 -0.735383   64.741541\n",
       "6  tL6pL  0.645075  0.530656  1.780266   49.055285\n",
       "7  BYPU6 -0.400648  0.808337 -5.624670   72.943292\n",
       "8  j9Oui  0.643105 -0.551583  2.372141  113.356160\n",
       "9  OLuZU  2.173381  0.563698  9.441852  127.910945"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take initial look at data\n",
    "\n",
    "reg_data_0.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg 0 Shape: (100000, 5)\n",
      "Reg 1 Shape: (100000, 5)\n",
      "Reg 2 Shape: (100000, 5)\n"
     ]
    }
   ],
   "source": [
    "# Observe shape of data\n",
    "\n",
    "print(f\"Reg 0 Shape: {reg_data_0.shape}\")\n",
    "print(f\"Reg 1 Shape: {reg_data_1.shape}\")\n",
    "print(f\"Reg 2 Shape: {reg_data_2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region Data 0:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count   Dtype  \n",
      "---  ------   --------------   -----  \n",
      " 0   id       100000 non-null  object \n",
      " 1   f0       100000 non-null  float64\n",
      " 2   f1       100000 non-null  float64\n",
      " 3   f2       100000 non-null  float64\n",
      " 4   product  100000 non-null  float64\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 3.8+ MB\n",
      "\n",
      "\n",
      "\n",
      "Region Data 1:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count   Dtype  \n",
      "---  ------   --------------   -----  \n",
      " 0   id       100000 non-null  object \n",
      " 1   f0       100000 non-null  float64\n",
      " 2   f1       100000 non-null  float64\n",
      " 3   f2       100000 non-null  float64\n",
      " 4   product  100000 non-null  float64\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 3.8+ MB\n",
      "\n",
      "\n",
      "\n",
      "Region Data 2:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count   Dtype  \n",
      "---  ------   --------------   -----  \n",
      " 0   id       100000 non-null  object \n",
      " 1   f0       100000 non-null  float64\n",
      " 2   f1       100000 non-null  float64\n",
      " 3   f2       100000 non-null  float64\n",
      " 4   product  100000 non-null  float64\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 3.8+ MB\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look at general info for each data set\n",
    "\n",
    "datasets = (reg_data_0, reg_data_1, reg_data_2)\n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    print(f\"Region Data {i}:\")\n",
    "    datasets[i].info()\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates in Region Data 0: 10\n",
      "Number of duplicates in Region Data 1: 4\n",
      "Number of duplicates in Region Data 2: 4\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in ID column\n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    print(f\"Number of duplicates in Region Data {i}: {datasets[i]['id'].duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "The datasets `reg_data_0`, `reg_data_1`, and `reg_data_2` are identical in structure (shape), each containing no missing values and comprising 5 columns with consistent data types:\n",
    "- id: Object-type\n",
    "- f0: float64\n",
    "- f1: float64\n",
    "- f2: float64\n",
    "- product: float64\n",
    "\n",
    "Duplicate values are present in the `id` column across all three datasets. Given that these columns are unnecessary for the model, they can be safely removed. There are no duplicates in any other column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary column for model\n",
    "\n",
    "reg_data_0 = reg_data_0.drop(columns='id')\n",
    "reg_data_1 = reg_data_1.drop(columns='id')\n",
    "reg_data_2 = reg_data_2.drop(columns='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassign back to datasets list\n",
    "\n",
    "datasets = (reg_data_0, reg_data_1, reg_data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Summary:\n",
    "Each file corresponds to data from three distinct regions.\n",
    "\n",
    "- `id` -- a unique identifier for each oil well, has been removed as it does not contribute to model training.\n",
    "- `f0`, `f1`, `f2` -- represent various features of the oil wells and have consistent scaling.\n",
    "- `product` -- indicates the volume of reserves in each oil well (measured in thousand barrels).\n",
    "\n",
    "All files are now clear of duplicate entries and missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_3_1(dataset, drop_cols, target_col, test_s=0.25, rnd_state=123, shuffle=True, axis=1):\n",
    "    '''Prints a statement specifying the data-split used and returns 2 variables (features and target) for the train and validation datasets respectively'''\n",
    "    \n",
    "    # Define the features & target\n",
    "    features = dataset.drop(drop_cols, axis=axis)\n",
    "    target = dataset[target_col]\n",
    "    \n",
    "    # Split the source data into 25% for Validation and 75% for Training\n",
    "    features_train, features_valid, target_train, target_valid = train_test_split(features, target, test_size=test_s, shuffle=True, random_state=rnd_state)\n",
    "    \n",
    "    # Print confirmation of data split\n",
    "    sum_of_datasets = len(features_train) + len(features_valid) \n",
    "    if len(dataset) == sum_of_datasets:\n",
    "        print(f\"Features split ratio is 3:1, where data split is allocated as:\\n- Training = 75% [shape={features_train.shape}]\\n- Validation = 25% [shape={features_valid.shape}]\")\n",
    "        print(f\"Target split ratio is 3:1, where data split is allocated as:\\n- Training = 75% [shape={target_train.shape}]\\n- Validation = 25% [shape={target_valid.shape}]\")\n",
    "    \n",
    "    return features_train, features_valid, target_train, target_valid \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg 0:\n",
      "Features split ratio is 3:1, where data split is allocated as:\n",
      "- Training = 75% [shape=(75000, 3)]\n",
      "- Validation = 25% [shape=(25000, 3)]\n",
      "Target split ratio is 3:1, where data split is allocated as:\n",
      "- Training = 75% [shape=(75000,)]\n",
      "- Validation = 25% [shape=(25000,)]\n",
      "\n",
      "Reg 1:\n",
      "Features split ratio is 3:1, where data split is allocated as:\n",
      "- Training = 75% [shape=(75000, 3)]\n",
      "- Validation = 25% [shape=(25000, 3)]\n",
      "Target split ratio is 3:1, where data split is allocated as:\n",
      "- Training = 75% [shape=(75000,)]\n",
      "- Validation = 25% [shape=(25000,)]\n",
      "\n",
      "Reg 2:\n",
      "Features split ratio is 3:1, where data split is allocated as:\n",
      "- Training = 75% [shape=(75000, 3)]\n",
      "- Validation = 25% [shape=(25000, 3)]\n",
      "Target split ratio is 3:1, where data split is allocated as:\n",
      "- Training = 75% [shape=(75000,)]\n",
      "- Validation = 25% [shape=(25000,)]\n"
     ]
    }
   ],
   "source": [
    "# Define features & target for all three regions\n",
    "\n",
    "print(\"Reg 0:\")\n",
    "features_0_train, features_0_valid, target_0_train, target_0_valid = split_data_3_1(reg_data_0, ['product'], 'product')\n",
    "print()\n",
    "\n",
    "print(\"Reg 1:\")\n",
    "features_1_train, features_1_valid, target_1_train, target_1_valid = split_data_3_1(reg_data_1, ['product'], 'product')\n",
    "print()\n",
    "\n",
    "print(\"Reg 2:\")\n",
    "features_2_train, features_2_valid, target_2_train, target_2_valid = split_data_3_1(reg_data_2, ['product'], 'product')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate metrics\n",
    "\n",
    "def calculate_regression_metrics(x, y):\n",
    "    '''Accepts the target and predicted values and resturns several metrics'''\n",
    "    mse = mean_squared_error(x, y)\n",
    "    rmse = mean_squared_error(x, y, squared=False)\n",
    "    r2 = r2_score(x, y)\n",
    "    mae = mean_absolute_error(x, y)\n",
    "    \n",
    "    print(f\"MSE: {mse}\\nRMSE: {rmse}\\nR^2: {r2}\\nMAE: {mae}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for Region 0\n",
    "\n",
    "model_0 = LinearRegression()\n",
    "model_0.fit(features_0_train, target_0_train)\n",
    "predictions_0_valid = model_0.predict(features_0_valid)\n",
    "\n",
    "correct_values_0 = target_0_valid.tolist()\n",
    "predicted_values_0 = predictions_0_valid.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Region 1 Model\n",
    "\n",
    "model_1 = LinearRegression()\n",
    "model_1.fit(features_1_train, target_1_train)\n",
    "predictions_1_valid = model_1.predict(features_1_valid)\n",
    "\n",
    "correct_values_1 = target_1_valid.tolist()\n",
    "predicted_values_1 = predictions_1_valid.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Region 2 Model\n",
    "\n",
    "model_2 = LinearRegression()\n",
    "model_2.fit(features_2_train, target_2_train)\n",
    "predictions_2_valid = model_2.predict(features_2_valid)\n",
    "\n",
    "correct_values_2 = target_2_valid.tolist()\n",
    "predicted_values_2 = predictions_2_valid.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region 0 Average Volume of Predicted Reserve (thousand barrels): 92.54936189116307\n",
      "Region 1 Average Volume of Predicted Reserve (thousand barrels): 69.28001860653977\n",
      "Region 2 Average Volume of Predicted Reserve (thousand barrels): 95.09859933591373\n"
     ]
    }
   ],
   "source": [
    "# Average number of predicted reserves\n",
    "\n",
    "print(f\"Region 0 Average Volume of Predicted Reserve (thousand barrels): {statistics.mean(predicted_values_0)}\")\n",
    "print(f\"Region 1 Average Volume of Predicted Reserve (thousand barrels): {statistics.mean(predicted_values_1)}\")\n",
    "print(f\"Region 2 Average Volume of Predicted Reserve (thousand barrels): {statistics.mean(predicted_values_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Region 0:\n",
      "MSE: 1417.3615751967832\n",
      "RMSE: 37.64786282376176\n",
      "R^2: 0.2812975228159569\n",
      "MAE: 30.984212391272273\n",
      "\n",
      "Metrics for Region 1:\n",
      "MSE: 0.8017661964648819\n",
      "RMSE: 0.8954139804944313\n",
      "R^2: 0.9996180923165817\n",
      "MAE: 0.7210648408937341\n",
      "\n",
      "Metrics for Region 2:\n",
      "MSE: 1610.2587969766078\n",
      "RMSE: 40.12803006598514\n",
      "R^2: 0.19313657905573023\n",
      "MAE: 32.80763017044863\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate Metrics for each model\n",
    "\n",
    "print(\"Metrics for Region 0:\")\n",
    "calculate_regression_metrics(target_0_valid, predictions_0_valid)\n",
    "\n",
    "print(\"Metrics for Region 1:\")\n",
    "calculate_regression_metrics(target_1_valid, predictions_1_valid)\n",
    "\n",
    "print(\"Metrics for Region 2:\")\n",
    "calculate_regression_metrics(target_2_valid, predictions_2_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "* The data indicates that Region 2 has the highest average reserve, followed by Region 0, and then Region 1 with the lowest.\n",
    "\n",
    "* Based on the Root Mean Squared Error (RMSE) metric, the RMSE value for Region 1 is notably lower than that for Region 0 or Region 2. This suggests that the model performed most accurately in Region 1 in terms of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
